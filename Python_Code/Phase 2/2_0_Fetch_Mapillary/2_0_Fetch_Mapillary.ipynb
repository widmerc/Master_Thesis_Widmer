{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1309c2b5",
   "metadata": {},
   "source": [
    "# Efficiently Querying and Processing Mapillary Data – Project Overview\n",
    "\n",
    "## Goal\n",
    "\n",
    "I aim to efficiently query, process, and enrich large volumes of Mapillary image data (over 1'200'000 images) using the Mapillary API. The goal is to extract metadata and save the results as GeoPackage files (GPKG) for further spatial analysis.\n",
    "\n",
    "Along the way, I encountered several bottlenecks — mainly due to API limitations (max 100 images per query, rate limits) and the sheer volume of data to process (Big Data Problems).\n",
    "\n",
    "## General Setup\n",
    "\n",
    "I use:\n",
    "\n",
    "- `ThreadPoolExecutor` for parallel execution\n",
    "- The Mapillary API:\n",
    "  - `images_in_bbox()` for spatial queries\n",
    "  - `graph.mapillary.com/{image_id}` for metadata\n",
    "- `GeoPandas` for geospatial data processing and saving as GPKG\n",
    "- Retry logic and HTTP session pooling for stable requests\n",
    "\n",
    "Input: Bounding box (GeoJSON or coordinates)  \n",
    "Output: Image and metadata stored as `.json` and `.gpkg`\n",
    "\n",
    "## Learning Steps – My Iterative Approach\n",
    "\n",
    "### General Benchmark – Single vs. Parallel Processing\n",
    "\n",
    "I tested both single-threaded and multi-threaded image processing using Python's `ThreadPoolExecutor`.\n",
    "\n",
    "| Mode       | Images | Duration     |\n",
    "|------------|--------|--------------|\n",
    "| Single     | 98     | 66.95 sec     |\n",
    "| Parallel   | 100    | 12.33 sec     |\n",
    "\n",
    "Parallel processing was up to 5 times faster. Threading works well here because the task is I/O-bound (API calls). However, rate limits and error handling needed careful management. I used this approach for the further analysis.\n",
    "\n",
    "### 1) Brute Force Grid with 10m Resolution\n",
    "\n",
    "**What I tried:**  \n",
    "I generated a fine grid (10m x 10m) and queried `images_in_bbox()` for each cell using multiprocessing.\n",
    "\n",
    "**What I learned:**  \n",
    "This approach caused an extreme number of API requests, even in empty areas. Since each tile returned max 100 images, it required a huge number of calls and quickly hit rate limits.\n",
    "\n",
    "**Problems encountered:**\n",
    "- Frequent HTTP 429 (too many requests)\n",
    "- Many unnecessary queries in empty areas\n",
    "- API only returns a maximum of 100 results, making fine-grained discovery incomplete\n",
    "- Extremely slow: would take **days** for the entire area\n",
    "\n",
    "### 2) Distributed Access Using Multiple API Tokens\n",
    "\n",
    "**What I tried:**  \n",
    "I assigned a separate API access token to each worker/thread to try increasing throughput.\n",
    "\n",
    "**What I learned:**  \n",
    "This did not help much. Even with multiple tokens, each tile still took ~0.3 seconds. For hundreds of thousands of tiles, the process remained unacceptably slow.\n",
    "\n",
    "**Problems encountered:**\n",
    "- Difficult token management\n",
    "- Rate limits still applied per token\n",
    "- No significant performance gain\n",
    "- Still made unnecessary calls to empty areas\n",
    "\n",
    "### 3) Hierarchical Grid (\"WMS Pyramid\" Strategy)\n",
    "\n",
    "**What I tried:**  \n",
    "Inspired by how WMS tiles work, I implemented a zoom-based grid system:\n",
    "\n",
    "1. Start with large grids (e.g. 1 km x 1 km)\n",
    "2. If image data exists: subdivide further (500m, 250m, …)\n",
    "3. Stop refining when no data is found\n",
    "4. For empty parent tiles, generate empty `.json` and skip children\n",
    "\n",
    "**What I learned:**  \n",
    "This drastically reduced the number of requests. I avoided unnecessary queries in empty areas, while still achieving fine resolution where needed.\n",
    "\n",
    "**Problems encountered:**\n",
    "- Slightly more complex to implement\n",
    "- Need to manage and track already-visited tiles\n",
    "- Edge tiles may still require careful merging if overlapping\n",
    "- The process time would be over 1 day\n",
    "\n",
    "### 4) Metadata Download via `graph.mapillary.com`\n",
    "\n",
    "**What I tried:**  \n",
    "After further research, I found the `Tiled Dataset` API, which could fetch basic information e.g. Image-ID and Sequence-ID over a certain area. This was very quick, approx 1 minute. After I had the necessary and reduced dataset with the geographical coorinates in a `.gpkg`, I could implement a request for each image for further metadata fetching.\n",
    "\n",
    "**Implementation steps:**\n",
    "\n",
    "1. Load image IDs from `.gpkg`\n",
    "2. For each ID, request metadata from  \n",
    "   `https://graph.mapillary.com/{image_id}?access_token=...`\n",
    "3. Collect results in a `pandas.DataFrame`\n",
    "4. Merge metadata with `GeoDataFrame`\n",
    "5. Save as final GPKG file\n",
    "\n",
    "**Optimizations:**\n",
    "\n",
    "- Retry logic with `urllib3` and `HTTPAdapter`\n",
    "- Parallelized via `ThreadPoolExecutor`\n",
    "- Request rate control:\n",
    "\n",
    "```python\n",
    "requests_per_minute = 50000\n",
    "safety_factor = 0.9\n",
    "min_delay = 60.0 / requests_per_minute\n",
    "max_workers = max(1, int(requests_per_minute * safety_factor * min_delay))\n",
    "```\n",
    "\n",
    "**What I learned:**  \n",
    "Controlling the request rate was essential. Even with 10,000+ threads, I was able to finish metadata downloading in just over 2 hours without hitting rate limits.\n",
    "\n",
    "**Problems encountered:**\n",
    "- If rate control was not correctly configured: immediate HTTP 429\n",
    "- Some image IDs failed to resolve even after retries (handled via logging)\n",
    "\n",
    "\n",
    "\n",
    "## Final Overview Table – Advantages and Issues per Step\n",
    "\n",
    "| Step | Strategy                                  | Advantages                                                                 | Problems / Errors Encountered                                                        |\n",
    "|------|-------------------------------------------|----------------------------------------------------------------------------|--------------------------------------------------------------------------------------|\n",
    "|  -   | General Benchmark (Parallel vs. Single)   | Fast metadata processing for known images                                  | Only works if image IDs are already available                                       |\n",
    "| 1    | Brute Force Grid (10m)                    | Simple to implement                                                        | API overload, rate limits, unnecessary calls, incomplete due to 100 image cap       |\n",
    "| 2    | Multiple Tokens per Worker                | Distributed access, theoretical speed gain                                 | No significant improvement, token management issues, still slow per tile            |\n",
    "| 3    | Hierarchical Grid (WMS-style refinement)  | Highly efficient, scalable, minimal API usage in empty areas               | More complex logic, need to track hierarchy, still needs fallback for edge cases    |\n",
    "| 4    | Metadata Download via Graph API           | Rich metadata, stable, fast with parallelization and throttling            | Sensitive to request rate, some failed IDs, needs retry mechanism                   |\n",
    "\n",
    "\n",
    "## Approximate Processing Times (Ryzen 7900X / RTX 4080 / 64 GB RAM / SSD)\n",
    "\n",
    "| Step | Strategy                                  | Estimated Time (Approx.) <- canceled after certain time                  |\n",
    "|------|-------------------------------------------|-------------------------------------------|\n",
    "| -    | General Benchmark (Parallel Mode)         | ~12 seconds for 100 images                |\n",
    "| 1    | Brute Force Grid (10m tiles, full area)   | Several days (infeasible)                 |\n",
    "| 2    | Multiple Tokens per Worker                | Still >24 hours for 900,000+ tiles        |\n",
    "| 3    | Hierarchical Grid (WMS-style refinement)  | ~6-9 hours for full area             |\n",
    "| 4    | Metadata Download via Graph API           | ~2 hours 8 minutes (actual, measured)     |\n",
    "\n",
    "\n",
    "## Code Overview – Fetching and Enriching Mapillary Data\n",
    "\n",
    "This script demonstrates how to use a custom module (`Mapillary_Fetch_Metadata.py`) to:\n",
    "\n",
    "1. Fetch basic Mapillary image data within a bounding polygon (GeoJSON-style coordinates)\n",
    "2. Enrich this data by downloading full metadata for each image\n",
    "3. Save the results as GeoPackages (`.gpkg`) and optionally log failed image IDs\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "* Both steps automatically **skip processing** if the output file already exists\n",
    "* `failed_ids.txt` logs any image IDs for which metadata download failed (e.g. due to API errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be39b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Relativer Pfad zum Script-Verzeichnis\n",
    "script_path = Path(\"03_Model/Scripts/2_Feature_Geolocation/2_0_Fetch_Mapillary\")\n",
    "sys.path.append(str(script_path.resolve()))\n",
    "\n",
    "# Relativer Pfad zum Export-Verzeichnis\n",
    "root_path = Path(r\"./.\").resolve()\n",
    "\n",
    "\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "\n",
    "def read_gpkg_limited(path, limit=None):\n",
    "    if limit is None:\n",
    "        return gpd.read_file(path)\n",
    "    \n",
    "    with fiona.open(path, layer=0) as src:\n",
    "        features = [feature for _, feature in zip(range(limit), src)]\n",
    "        return gpd.GeoDataFrame.from_features(features, crs=src.crs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45be6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic metadata already exists at C:\\Users\\claud\\Documents\\Studium\\Masterarbeit\\03_Model\\Scripts\\2_Feature_Geolocation\\2_0_Fetch_Mapillary\\data\\images_bbox_basic.gpkg. Skipping fetch.\n",
      "Metadata-Datei C:\\Users\\claud\\Documents\\Studium\\Masterarbeit\\03_Model\\Scripts\\2_Feature_Geolocation\\2_0_Fetch_Mapillary\\data\\images_bbox_fullmeta.gpkg existiert bereits - wird übersprungen.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Mapillary_Fetch_Metadata import fetch_and_convert_to_gdf\n",
    "from Mapillary_Fetch_Metadata import load_and_fetch_metadata\n",
    "import geopandas as gpd\n",
    "\n",
    "# Example bounding box coordinates for testing\n",
    "\n",
    "bbox_extent = [\n",
    "        [8.457720822843683, 47.390649130286448], \n",
    "        [8.456825005154048, 47.37424440909907], \n",
    "        [8.513957476598119, 47.334770736179166], \n",
    "        [8.538369416730317, 47.347789816857421], \n",
    "        [8.566987407296029, 47.345832805990639], \n",
    "        [8.578446117577144, 47.351059452649729], \n",
    "        [8.615089558171336, 47.363347841537582], \n",
    "        [8.593951036370918, 47.38175404740641], \n",
    "        [8.598868228275775, 47.407016330860856], \n",
    "        [8.574228138587589, 47.412600163192216], \n",
    "        [8.55660690453244, 47.419692963652167], \n",
    "        [8.556685022028232, 47.437291328914533], \n",
    "        [8.514688737203723, 47.435910038955718], \n",
    "        [8.476965065062107, 47.42933398525328], \n",
    "        [8.466429431361696, 47.419442231388643], \n",
    "        [8.457720822843683, 47.390649130286448]\n",
    "    ]\n",
    "    \n",
    "    # File paths\n",
    "basic_output_path = root_path / \"data\" / \"images_bbox_basic.gpkg\"\n",
    "full_output_path = root_path/ \"data\" / \"images_bbox_fullmeta.gpkg\"\n",
    "failed_ids_path = root_path/ \"data\" / \"failed_ids.txt\"\n",
    "\n",
    "# Fetch basic metadata and save to GeoPackage\n",
    "if not basic_output_path.exists():\n",
    "    print(f\"Fetching basic metadata for bounding box: {bbox_extent}\")\n",
    "    # Fetch basic metadata and convert to GeoDataFrame\n",
    "    fetch_and_convert_to_gdf(\n",
    "    bbox=bbox_extent, \n",
    "    output_path=basic_output_path\n",
    "    )\n",
    "else:\n",
    "    print(f\"Basic metadata already exists at {basic_output_path}. Skipping fetch.\")\n",
    "\n",
    "    # Fetch full metadata (with skip check and custom failed_ids path)\n",
    "merged_gdf = load_and_fetch_metadata(\n",
    "        basic_output_path, \n",
    "        full_output_path, \n",
    "        limit=1000,\n",
    "        failed_ids_path=failed_ids_path\n",
    "    )\n",
    "\n",
    "del merged_gdf\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bc6df",
   "metadata": {},
   "source": [
    "### Blur Detection of Images\n",
    "\n",
    "After enriching metadata, I want to **check each image for blur**, using both Laplacian and Sobel methods.\n",
    "\n",
    "* The images will be **downloaded permantly** and stored in RAM or a temp folder. (in an extern SSD)\n",
    "\n",
    "The following imported script `detect_blury_image.py` provides the blur detection logic:\n",
    "\n",
    "* `laplacian_variance_blur(...)`: Laplacian-based blur detection\n",
    "* `sobel_variance_blur(...)`: Sobel-based blur detection\n",
    "* `detect_blurry_images_in_folder(...)`: Batch blur detection in a folder (optionally with visualization)\n",
    "* `detect_blury_image_single(...)`: For single image evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee83a3",
   "metadata": {},
   "source": [
    "**Date:** 2025-06-26  \n",
    "\n",
    "#### Overview\n",
    "\n",
    "Started with a script that:\n",
    "- Loaded a `.gpkg` GeoDataFrame with image URLs (`thumb_1024_url`)\n",
    "- Processed images in chunks of 1000\n",
    "- Detected blur using Laplacian and Sobel methods\n",
    "- Implemented a basic version **without multiprocessing**\n",
    "- Images were downloaded sequentially\n",
    "- Blur detection ran in a single-threaded loop\n",
    "\n",
    "#### Revisions:\n",
    "- ✅ **Chunk-wise processing** for memory-safe handling of large GeoPackages\n",
    "- ✅ **Parallel image download** using `ThreadPoolExecutor`\n",
    "- ✅ **Parallel blur detection** using `ThreadPoolExecutor` inside `detect_blurry_images_in_folder()`\n",
    "- ✅ **Switch to `aiohttp` + `asyncio`** for faster, asynchronous image downloads (maximizing network throughput)\n",
    "- ✅ **Temporary folders** per chunk to store images and reduce disk/RAM pressure\n",
    "- ✅ **Efficient in-place updates** to the GeoDataFrame (`blurry_image = True/False`)\n",
    "- ✅ **Interim statistics** after each chunk (blurred / not blurred / failed)\n",
    "- ✅ **Final `.gpkg` export** with updated blur status\n",
    "- ✅ **Save failed download** URLs to retry later\n",
    "- ✅ **Changed blur detection algorithm**: from 39.2 seconds to 27 seconds\n",
    "- ✅ **Added GPU support**: from 27 seconds to 24 seconds for blur detection\n",
    "- ✅ **Added asynchronous processing**: from 57 seconds to 27 seconds for downloading **and** blur detection of 10,000 images (~54 min for 1.2 million images) \n",
    "- ✅ **Optimized GPU-based blur detection pipeline** (→ GPU runs in parallel with image downloading, .loc instead of .at for faster GeoDataFrame updates, GPU memory freed after each chunk) -> from 27 seconds to 19 seconds  -> but it \"hungers\" after the first iteration\n",
    "- ✅ **Changed image download from temporary to definitive**: Blur detection is massively increased by it.\n",
    "- ✅ **Changed blur detection output**: Changed that the values for the blur detection are in the gpkg.\n",
    "- ✅ **Added Testing Method**: Added a way to test only with a subsample of the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25da6985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1194448 Bilder sind bereits gespeichert.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download: 100%|██████████| 40103/40103 [00:21<00:00, 1867.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ Gesamtdauer für den Download: 89.20 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# === Standard Library Imports ===\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import geopandas as gpd\n",
    "from tqdm.asyncio import tqdm as tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import nest_asyncio\n",
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "from config import (\n",
    "    IMAGE_COLUMN,\n",
    "    BLURRY_COL,\n",
    "    GPKG_PATH,\n",
    "    OUTPUT_GPKG,\n",
    "    FAILED_DOWNLOADS_PATH,\n",
    "    TMP_BLUR_PATH,\n",
    "    IMAGE_DIR,\n",
    "    TEST_LIMIT,\n",
    "    LAPLACIAN_THRESHOLD,\n",
    "    USE_GPU\n",
    ")\n",
    "from blur_gpu_utils import compute_laplacian_variance\n",
    "\n",
    "\n",
    "\n",
    "# === Initialisierung ===\n",
    "nest_asyncio.apply()\n",
    "TMP_BLUR_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "# === Download-Funktionen ===\n",
    "async def download_image(session, sem, image_id, url, folder):\n",
    "    filename = f\"{image_id}.jpg\"\n",
    "    save_path = os.path.join(folder, filename)\n",
    "    try:\n",
    "        async with sem:\n",
    "            async with session.get(url, timeout=0.5) as resp:\n",
    "                if resp.status == 200:\n",
    "                    async with aiofiles.open(save_path, mode='wb') as f:\n",
    "                        await f.write(await resp.read())\n",
    "                    return None\n",
    "    except:\n",
    "        pass\n",
    "    return (image_id, url)\n",
    "\n",
    "async def download_all_images(idx_url_list, folder, max_connections=100):\n",
    "    sem = asyncio.Semaphore(max_connections)\n",
    "    async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit_per_host=max_connections)) as session:\n",
    "        tasks = [download_image(session, sem, image_id, url, folder) for image_id, url in idx_url_list]\n",
    "        results = await tqdm_asyncio.gather(*tasks, desc=\"Download\", total=len(tasks))\n",
    "    return [r for r in results if r is not None]\n",
    "\n",
    "async def download_all_images_to_directory(gdf, directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    gdf[\"already_downloaded\"] = gdf[\"id\"].map(lambda image_id: os.path.exists(os.path.join(directory, f\"{image_id}.jpg\")))\n",
    "    already_downloaded_count = gdf[\"already_downloaded\"].sum()\n",
    "    print(f\"✅ {already_downloaded_count} Bilder sind bereits gespeichert.\")\n",
    "\n",
    "    remaining_gdf = gdf[~gdf[\"already_downloaded\"]]\n",
    "\n",
    "    if remaining_gdf.empty:\n",
    "        print(\"✅ Alle Bilder sind bereits heruntergeladen.\")\n",
    "        return gdf\n",
    "\n",
    "    idx_url_list = list(zip(remaining_gdf[\"id\"], remaining_gdf[IMAGE_COLUMN]))\n",
    "    await download_all_images(idx_url_list, directory)\n",
    "\n",
    "    gdf.drop(columns=[\"already_downloaded\"], inplace=True)\n",
    "    return gdf\n",
    "\n",
    "# === Hauptausführung ===\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    gdf = read_gpkg_limited(GPKG_PATH, TEST_LIMIT)\n",
    "    gdf[BLURRY_COL] = None\n",
    "\n",
    "    gdf = asyncio.run(download_all_images_to_directory(gdf, IMAGE_DIR))\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"⏱️ Gesamtdauer für den Download: {elapsed:.2f} Sekunden\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c0138a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Starte Blur Detection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "📸 Blur Detection (parallel): 100%|██████████| 1234551/1234551 [25:53<00:00, 794.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blur Detection abgeschlossen in 1579.35 Sekunden.\n",
      "💾 GPKG gespeichert: data\\images_bbox_fullmeta_with_blur.gpkg\n"
     ]
    }
   ],
   "source": [
    "# Lade GeoDataFrame\n",
    "gdf = read_gpkg_limited(GPKG_PATH, TEST_LIMIT)\n",
    "\n",
    "\n",
    "print(\"🔍 Starte Blur Detection...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Spalten vorbereiten\n",
    "gdf[\"blur_value\"] = None\n",
    "gdf[\"is_blurry\"] = None\n",
    "\n",
    "def get_image_path(image_id):\n",
    "    return os.path.join(IMAGE_DIR, f\"{image_id}.jpg\")\n",
    "\n",
    "# Hauptloop\n",
    "from blur_gpu_utils import batch_compute_blur\n",
    "\n",
    "# Bildpfade vorbereiten\n",
    "image_paths = [\n",
    "    (str(image_id), os.path.join(IMAGE_DIR, f\"{image_id}.jpg\"))\n",
    "    for image_id in gdf[\"id\"]\n",
    "]\n",
    "# Parallele Berechnung\n",
    "results = batch_compute_blur(image_paths, use_gpu=USE_GPU, max_workers=16)\n",
    "\n",
    "# Ergebnisse zuordnen\n",
    "blur_map = {img_id: val for img_id, val in results}\n",
    "gdf[\"blur_value\"] = gdf[\"id\"].astype(str).map(blur_map)\n",
    "gdf[\"is_blurry\"] = gdf[\"blur_value\"] < LAPLACIAN_THRESHOLD\n",
    "\n",
    "\n",
    "print(f\"✅ Blur Detection abgeschlossen in {time.time() - start_time:.2f} Sekunden.\")\n",
    "\n",
    "# Speichern\n",
    "gdf.to_file(OUTPUT_GPKG, driver=\"GPKG\")\n",
    "print(f\"💾 GPKG gespeichert: {OUTPUT_GPKG}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-master-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
