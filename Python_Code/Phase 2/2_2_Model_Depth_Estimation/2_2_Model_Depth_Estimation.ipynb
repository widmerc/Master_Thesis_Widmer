{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b90693f9",
   "metadata": {},
   "source": [
    "# Depth Estimation\n",
    "\n",
    "https://huggingface.co/tasks/depth-estimation\n",
    "\n",
    "@inproceedings{depthanything,\n",
    "  title={Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data},\n",
    "  author={Yang, Lihe and Kang, Bingyi and Huang, Zilong and Xu, Xiaogang and Feng, Jiashi and Zhao, Hengshuang},\n",
    "  booktitle={CVPR},\n",
    "  year={2024}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b770de2",
   "metadata": {},
   "source": [
    "\n",
    "Before performing depth estimation, we first need to:\n",
    "1. List all .jpg image files in the dataset folder\n",
    "2. Validate each image using PIL to ensure it's readable and not corrupted\n",
    "Only valid images will be used for depth estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "361f8528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning folder for .jpg files... (Input: D:\\Mapillary_Data)\n",
      "1,000,000 .jpg files found...\n",
      "Done. Total of 1,194,448 .jpg files saved in 'D:\\Mapillary_Data_list.txt'.\n",
      "Done. File saved: D:\\Mapillary_Data_list.txt\n",
      "Validating images using PIL (parallel)...\n",
      "Validating 1,194,448 images with PIL (parallel)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Image validation: 100%|██████████| 1194448/1194448 [01:29<00:00, 13296.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,187,364 valid images saved in: D:\\Mapillary_Data_valid_list.txt\n",
      "7,084 invalid images detected.\n",
      "Done. Valid image list saved to: D:\\Mapillary_Data_valid_list.txt\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import config\n",
    "from folder_scanner import stream_jpg_paths, validate_images_parallel\n",
    "\n",
    "from config import ROOT_PATH, DATA_PATH, MODEL_PATH, IMAGE_DIR\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_folder = IMAGE_DIR\n",
    "    jpg_list_txt = IMAGE_DIR + \"_list.txt\"\n",
    "    valid_list_txt = IMAGE_DIR + \"_valid_list.txt\"\n",
    "\n",
    "    # Step 1: Scan the folder for .jpg files\n",
    "    print(f\"Scanning folder for .jpg files... (Input: {input_folder})\")\n",
    "    stream_jpg_paths(input_folder, jpg_list_txt)\n",
    "    print(f\"Done. File saved: {jpg_list_txt}\")\n",
    "\n",
    "    # Step 2: Validate image files using PIL\n",
    "    print(\"Validating images using PIL (parallel)...\")\n",
    "    validate_images_parallel(jpg_list_txt, valid_list_txt, num_workers=None)\n",
    "    print(f\"Done. Valid image list saved to: {valid_list_txt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00f647",
   "metadata": {},
   "source": [
    "3. Reduce the resolution of the images because of computation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5e3889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downscaling 1,187,364 images to half size (skipping existing)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Resizing: 100%|██████████| 1187364/1187364 [01:05<00:00, 18227.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. 1,187,364 processed. 0 failed (others were skipped).\n"
     ]
    }
   ],
   "source": [
    "from config import IMAGE_DIR\n",
    "from pathlib import Path\n",
    "from downscale import downscale_and_save_valid_images\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ...\n",
    "    # Step 3: Downscale and save to IMAGE_DIR_red\n",
    "    output_folder = str(Path(IMAGE_DIR) .with_name(Path(IMAGE_DIR).name + \"_red\"))\n",
    "    downscale_and_save_valid_images(valid_list_txt, IMAGE_DIR, output_folder, num_workers=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3eb2e7",
   "metadata": {},
   "source": [
    "4. Depth Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa3eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1,187,364 images using model: depth-anything/Depth-Anything-V2-Small-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Depth Estimation:   5%|▌         | 62500/1187364 [10:29<7:22:53, 42.33it/s] You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Depth Estimation:  60%|██████    | 717500/1187364 [5:17:45<2:50:42, 45.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in batch 715000: cannot identify image file 'D:\\\\Mapillary_Data_red\\\\4033407186697923.jpg'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Depth Estimation: 100%|██████████| 1187364/1187364 [9:07:46<00:00, 36.13it/s]  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from config import IMAGE_DIR, DATA_PATH\n",
    "from depth_processing import process_from_valid_list\n",
    "\n",
    "\n",
    "\n",
    "original_folder = IMAGE_DIR\n",
    "reduced_folder = rf\"{IMAGE_DIR}_red\"\n",
    "output_folder = (DATA_PATH / \"depth_processed\").absolute()\n",
    "\n",
    "valid_txt_file = rf\"{IMAGE_DIR}_valid_list.txt\"\n",
    "\n",
    "process_from_valid_list(\n",
    "        valid_list_file=valid_txt_file,\n",
    "        original_root=original_folder,\n",
    "        reduced_root=reduced_folder,\n",
    "        output_folder=output_folder,\n",
    "        batch_size=2500,\n",
    "        save_images=False\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-master-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
