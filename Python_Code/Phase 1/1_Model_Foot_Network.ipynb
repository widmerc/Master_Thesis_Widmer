{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c426f4",
   "metadata": {},
   "source": [
    "# Get the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1677a9",
   "metadata": {},
   "source": [
    "\n",
    "## Download latest OSM Network from Geofabrik and clip it to Zurich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cf3e629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 12:42:19,629 - INFO - Loading Zurich PBF (canton-wide extract)…\n",
      "2025-08-23 12:42:41,367 - INFO - Using FULL city boundary (admin_level=8).\n",
      "2025-08-23 12:42:41,367 - INFO - Reloading OSM with bounding polygon…\n",
      "2025-08-23 12:42:41,384 - INFO - Extracting walking network (edges only, with 'access')…\n",
      "c:\\Users\\claud\\anaconda3\\envs\\conda-qgis\\Lib\\site-packages\\pyrosm\\networks.py:37: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  edges, nodes = prepare_geodataframe(\n",
      "2025-08-23 12:43:15,502 - INFO - Extracted 61140 edges (before filtering).\n",
      "2025-08-23 12:43:15,852 - INFO - Filtering edges by 'highway' and 'access' (vectorized)…\n",
      "2025-08-23 12:43:15,902 - INFO - Kept 42891 edges after highway filtering (from 61140).\n",
      "2025-08-23 12:43:15,919 - INFO - Kept 41496 edges after access filtering (from 42891).\n",
      "2025-08-23 12:43:15,919 - INFO - Sanitizing attributes (lists/dicts → strings)…\n",
      "2025-08-23 12:43:16,202 - INFO - Writing GeoPackage: ./data/working_data/zurich_walking_edges_filtered.gpkg :: edges\n",
      "2025-08-23 12:43:16,719 - INFO - Created 41,496 records\n",
      "2025-08-23 12:43:16,776 - INFO - Done.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pyrosm import OSM, get_data\n",
    "import time\n",
    "\n",
    "# ------------ Config\n",
    "START = time.time()\n",
    "IS_TEST = False\n",
    "TEST_GPKG = \"./data/working_data/is_test.gpkg\"\n",
    "TEST_AREA = 1000\n",
    "OUT_GPKG = \"./data/working_data/zurich_walking_edges_filtered.gpkg\"\n",
    "OUT_LAYER = \"edges\"\n",
    "\n",
    "# ------------ Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# ------------ Helper: shared test polygon (no disk writes)\n",
    "def compute_test_polygon(fp_pbf, name=\"Zürich\", radius_m=500, target_epsg=2056):\n",
    "    osm_tmp = OSM(fp_pbf)\n",
    "    boundary = osm_tmp.get_boundaries(name=name, boundary_type=\"administrative\")\n",
    "    if boundary is None or len(boundary) == 0:\n",
    "        raise RuntimeError(f\"No administrative boundary found for {name}.\")\n",
    "    boundary_city = boundary[boundary[\"admin_level\"] == \"8\"].iloc[0]\n",
    "\n",
    "    # buffer in LV95, then convert back to boundary CRS\n",
    "    city_poly = gpd.GeoSeries([boundary_city.geometry], crs=boundary.crs).to_crs(target_epsg)\n",
    "    center = city_poly.centroid.iloc[0]\n",
    "    test_area = center.buffer(radius_m)\n",
    "    poly = gpd.GeoSeries([test_area], crs=target_epsg).to_crs(boundary.crs).iloc[0]\n",
    "    return poly\n",
    "\n",
    "# ------------ 1) Fetch PBF & determine bounding polygon\n",
    "logging.info(\"Loading Zurich PBF (canton-wide extract)…\")\n",
    "fp = get_data(\"Zuerich\", directory=\"./data\")\n",
    "\n",
    "if IS_TEST:\n",
    "    poly = compute_test_polygon(fp, name=\"Zürich\", radius_m=TEST_AREA, target_epsg=2056)\n",
    "    logging.info(\"Using TEST AREA around city center (radius ~%dm).\", TEST_AREA)\n",
    "else:\n",
    "    osm_tmp = OSM(fp)\n",
    "    boundary = osm_tmp.get_boundaries(name=\"Zürich\", boundary_type=\"administrative\")\n",
    "    boundary_city = boundary[boundary[\"admin_level\"] == \"8\"].iloc[0]\n",
    "    poly = boundary_city.geometry\n",
    "    logging.info(\"Using FULL city boundary (admin_level=8).\")\n",
    "\n",
    "# ------------ 2) Load OSM with bounding polygon & extract walking network\n",
    "logging.info(\"Reloading OSM with bounding polygon…\")\n",
    "osm = OSM(fp, bounding_box=poly)\n",
    "\n",
    "logging.info(\"Extracting walking network (edges only, with 'access')…\")\n",
    "edges = osm.get_network(network_type=\"walking\", nodes=False, extra_attributes=[\"access\"])\n",
    "logging.info(\"Extracted %d edges (before filtering).\", len(edges))\n",
    "\n",
    "# to LV95\n",
    "edges = edges.to_crs(2056)\n",
    "\n",
    "# ------------ 3) Vectorized filtering\n",
    "logging.info(\"Filtering edges by 'highway' and 'access' (vectorized)…\")\n",
    "\n",
    "strict_hw = [\"footway\", \"path\", \"pedestrian\", \"steps\"]\n",
    "optional_hw = [\"living_street\", \"corridor\", \"platform\"]\n",
    "keep_hw = set(strict_hw + optional_hw)\n",
    "\n",
    "access_keep = [\"agricultural\", \"destination\", \"no\", \"permissive\", \"yes\", None]\n",
    "keep_access = set(access_keep)\n",
    "\n",
    "def _to_scalar(v):\n",
    "    if isinstance(v, (list, tuple, set)):\n",
    "        try:\n",
    "            return next(iter(v)) if len(v) > 0 else pd.NA\n",
    "        except TypeError:\n",
    "            return next(iter(v), pd.NA)\n",
    "    return v\n",
    "\n",
    "if \"highway\" in edges.columns:\n",
    "    edges[\"highway\"] = edges[\"highway\"].map(_to_scalar)\n",
    "else:\n",
    "    logging.warning(\"No 'highway' column found – skipping highway filter.\")\n",
    "\n",
    "if \"access\" in edges.columns:\n",
    "    edges[\"access\"] = edges[\"access\"].map(_to_scalar)\n",
    "else:\n",
    "    logging.warning(\"No 'access' column found – skipping access filter.\")\n",
    "\n",
    "if \"highway\" in edges.columns:\n",
    "    before = len(edges)\n",
    "    edges = edges[edges[\"highway\"].isin(keep_hw)]\n",
    "    logging.info(\"Kept %d edges after highway filtering (from %d).\", len(edges), before)\n",
    "\n",
    "if \"access\" in edges.columns:\n",
    "    before = len(edges)\n",
    "    mask_access = edges[\"access\"].isin(keep_access)\n",
    "    if None in keep_access:\n",
    "        mask_access = mask_access | edges[\"access\"].isna()\n",
    "    edges = edges[mask_access]\n",
    "    logging.info(\"Kept %d edges after access filtering (from %d).\", len(edges), before)\n",
    "\n",
    "# ------------ 4) Sanitize attributes & compute lengths\n",
    "logging.info(\"Sanitizing attributes (lists/dicts → strings)…\")\n",
    "def sanitize_columns(gdf):\n",
    "    for col in gdf.columns:\n",
    "        if col == gdf.geometry.name:\n",
    "            continue\n",
    "        gdf[col] = gdf[col].apply(\n",
    "            lambda v: json.dumps(v, ensure_ascii=False)\n",
    "            if isinstance(v, (dict, list, set, tuple)) else v\n",
    "        )\n",
    "    return gdf\n",
    "\n",
    "edges = sanitize_columns(edges)\n",
    "edges[\"length_m\"] = edges.geometry.length\n",
    "\n",
    "# ------------ 5) Write\n",
    "logging.info(\"Writing GeoPackage: %s :: %s\", OUT_GPKG, OUT_LAYER)\n",
    "edges.to_file(OUT_GPKG, layer=OUT_LAYER, driver=\"GPKG\")\n",
    "logging.info(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b5216",
   "metadata": {},
   "source": [
    "# Combine Data from City of Zurich and OSM to one walking network\n",
    "\n",
    "1. 1.5 m buffer from OSM and Zurich street network (square buffer ends)\n",
    "2. Union of both\n",
    "3. Fill holes smaller than 10 m²\n",
    "4. -0.5 m square buffer\n",
    "5. Merge multipart features into single parts\n",
    "6. Keep the longest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b6066",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "from pyrosm import OSM, get_data\n",
    "\n",
    "# ------------ Config\n",
    "CRS_TARGET = 2056\n",
    "\n",
    "OSM_EDGES_GPKG = \"./data/working_data/zurich_walking_edges_filtered.gpkg\"\n",
    "OSM_EDGES_LAYER = \"edges\"\n",
    "\n",
    "ZURICH_STREETS_GPKG = \"./data/input_data/Routing_City_Network.gpkg\"\n",
    "ZURICH_STREETS_LAYER = \"netz_fuss\"\n",
    "\n",
    "OUT_GPKG = \"./data/working_data/zurich_walk_network_union.gpkg\"\n",
    "OUT_LAYER = \"walk_union\"\n",
    "\n",
    "# Buffer parameters\n",
    "BUF_POS = 1.5\n",
    "BUF_NEG = -0.5\n",
    "HOLE_MAX_AREA = 15\n",
    "\n",
    "# ------------ Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "def ensure_crs(gdf: gpd.GeoDataFrame, crs_epsg: int) -> gpd.GeoDataFrame:\n",
    "    if gdf.crs is None:\n",
    "        raise ValueError(\"Input GeoDataFrame has no CRS. Please set it first.\")\n",
    "    if gdf.crs.to_epsg() != crs_epsg:\n",
    "        return gdf.to_crs(crs_epsg)\n",
    "    return gdf\n",
    "\n",
    "# same test polygon as Script 1 — no disk writes\n",
    "def compute_test_polygon(fp_pbf, name=\"Zürich\", radius_m=500, target_epsg=2056):\n",
    "    osm_tmp = OSM(fp_pbf)\n",
    "    boundary = osm_tmp.get_boundaries(name=name, boundary_type=\"administrative\")\n",
    "    if boundary is None or len(boundary) == 0:\n",
    "        raise RuntimeError(f\"No administrative boundary found for {name}.\")\n",
    "    boundary_city = boundary[boundary[\"admin_level\"] == \"8\"].iloc[0]\n",
    "    city_poly = gpd.GeoSeries([boundary_city.geometry], crs=boundary.crs).to_crs(target_epsg)\n",
    "    center = city_poly.centroid.iloc[0]\n",
    "    test_area = center.buffer(radius_m)\n",
    "    poly = gpd.GeoSeries([test_area], crs=target_epsg).iloc[0]\n",
    "    return poly  # already in target_epsg\n",
    "\n",
    "def remove_small_holes(geom, max_area: float):\n",
    "    if geom is None or geom.is_empty:\n",
    "        return geom\n",
    "    if geom.geom_type == \"Polygon\":\n",
    "        kept_interiors = []\n",
    "        for interior in geom.interiors:\n",
    "            hole_poly = Polygon(interior)\n",
    "            if hole_poly.area >= max_area:\n",
    "                kept_interiors.append(interior)\n",
    "        return Polygon(geom.exterior, kept_interiors)\n",
    "    elif geom.geom_type == \"MultiPolygon\":\n",
    "        parts = [remove_small_holes(p, max_area) for p in geom.geoms]\n",
    "        return unary_union([p for p in parts if p and not p.is_empty])\n",
    "    else:\n",
    "        return geom\n",
    "\n",
    "def save_layer(gdf: gpd.GeoDataFrame, layer: str):\n",
    "    \"\"\"Save intermediate layers to a single GPKG when IS_TEST=True.\"\"\"\n",
    "    gdf.to_file(TEST_GPKG, layer=layer, driver=\"GPKG\")\n",
    "    logging.info(\"Wrote test layer: %s (%d features)\", layer, len(gdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989a368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 12:43:16,793 - INFO - Loading OSM edges…\n",
      "2025-08-23 12:43:17,403 - INFO - Loading Zurich street network…\n",
      "2025-08-23 12:43:17,991 - INFO - Created 41,496 records\n",
      "2025-08-23 12:43:18,036 - INFO - Wrote test layer: 1_osm_edges (41496 features)\n",
      "2025-08-23 12:43:18,170 - INFO - Created 30,853 records\n",
      "2025-08-23 12:43:18,234 - INFO - Wrote test layer: 1_zrh_edges (30853 features)\n",
      "2025-08-23 12:43:18,234 - INFO - Buffering OSM edges (+1.50m)…\n",
      "2025-08-23 12:43:23,435 - INFO - Created 41,496 records\n",
      "2025-08-23 12:43:23,568 - INFO - Wrote test layer: 2_buf_osm (41496 features)\n",
      "2025-08-23 12:43:23,568 - INFO - Buffering ZRH edges (+1.50m)…\n",
      "2025-08-23 12:43:24,185 - INFO - Created 30,853 records\n",
      "2025-08-23 12:43:24,285 - INFO - Wrote test layer: 2_buf_zrh (30853 features)\n",
      "2025-08-23 12:43:24,285 - INFO - Union of both buffers…\n",
      "2025-08-23 12:43:48,653 - INFO - Created 1 records\n",
      "2025-08-23 12:43:48,704 - INFO - Wrote test layer: 3_union (1 features)\n",
      "2025-08-23 12:43:48,704 - INFO - Removing interior holes smaller than 15.0 m²…\n",
      "2025-08-23 12:43:59,374 - INFO - Created 1 records\n",
      "2025-08-23 12:43:59,431 - INFO - Wrote test layer: 4_union_noholes (1 features)\n",
      "2025-08-23 12:43:59,431 - INFO - Applying negative buffer -0.50m…\n",
      "2025-08-23 12:45:24,462 - INFO - Created 1 records\n",
      "2025-08-23 12:45:24,544 - INFO - Wrote test layer: 5_contracted (1 features)\n",
      "2025-08-23 12:45:24,544 - INFO - Exploding multipart geometries…\n",
      "2025-08-23 12:45:24,954 - INFO - Created 763 records\n",
      "2025-08-23 12:45:25,015 - INFO - Wrote test layer: 6_exploded (763 features)\n",
      "2025-08-23 12:45:25,382 - INFO - Created 1 records\n",
      "2025-08-23 12:45:25,447 - INFO - Wrote test layer: 7_top1 (1 features)\n",
      "2025-08-23 12:45:25,447 - INFO - Simplifying geometry (tolerance=0.5)…\n",
      "2025-08-23 12:46:16,419 - INFO - Created 1 records\n",
      "2025-08-23 12:46:16,448 - INFO - Wrote test layer: 7_simplified (1 features)\n",
      "2025-08-23 12:46:16,448 - INFO - Writing final result: ./data/working_data/zurich_walk_network_union.gpkg :: walk_union\n",
      "2025-08-23 12:46:16,486 - INFO - Created 1 records\n",
      "2025-08-23 12:46:16,501 - INFO - Done. 1 polygon(s) written.\n"
     ]
    }
   ],
   "source": [
    "# ------------ 1) Load data\n",
    "logging.info(\"Loading OSM edges…\")\n",
    "gdf_osm = gpd.read_file(OSM_EDGES_GPKG, layer=OSM_EDGES_LAYER)\n",
    "gdf_osm = ensure_crs(gdf_osm, CRS_TARGET)\n",
    "\n",
    "logging.info(\"Loading Zurich street network…\")\n",
    "gdf_zrh = gpd.read_file(ZURICH_STREETS_GPKG, layer=ZURICH_STREETS_LAYER)\n",
    "gdf_zrh = ensure_crs(gdf_zrh, CRS_TARGET)\n",
    "\n",
    "# ------------ 1a) Optional test area: identical to Script 1\n",
    "if IS_TEST:\n",
    "    logging.info(\"Clipping both layers with identical TEST AREA (radius ~%dm)…\", TEST_AREA)\n",
    "    fp = get_data(\"Zuerich\", directory=\"./data\")\n",
    "    test_area = compute_test_polygon(fp, name=\"Zürich\", radius_m=TEST_AREA, target_epsg=CRS_TARGET)\n",
    "    gdf_osm = gdf_osm.clip(test_area)\n",
    "    gdf_zrh = gdf_zrh.clip(test_area)\n",
    "\n",
    "# Save stage 1 layers\n",
    "save_layer(gdf_osm, \"1_osm_edges\")\n",
    "save_layer(gdf_zrh, \"1_zrh_edges\")\n",
    "\n",
    "# ------------ 2) Buffers\n",
    "logging.info(\"Buffering OSM edges (+%.2fm)…\", BUF_POS)\n",
    "gdf_buf_osm = gpd.GeoDataFrame(geometry=gdf_osm.geometry.buffer(BUF_POS), crs=CRS_TARGET)\n",
    "save_layer(gdf_buf_osm, \"2_buf_osm\")\n",
    "\n",
    "logging.info(\"Buffering ZRH edges (+%.2fm)…\", BUF_POS)\n",
    "gdf_buf_zrh = gpd.GeoDataFrame(geometry=gdf_zrh.geometry.buffer(BUF_POS), crs=CRS_TARGET)\n",
    "save_layer(gdf_buf_zrh, \"2_buf_zrh\")\n",
    "\n",
    "# ------------ 3) Union\n",
    "logging.info(\"Union of both buffers…\")\n",
    "union_geom = unary_union(list(gdf_buf_osm.geometry) + list(gdf_buf_zrh.geometry))\n",
    "gdf_union = gpd.GeoDataFrame(geometry=[union_geom], crs=CRS_TARGET)\n",
    "save_layer(gdf_union, \"3_union\")\n",
    "\n",
    "# ------------ 4) Remove small holes\n",
    "logging.info(\"Removing interior holes smaller than %.1f m²…\", HOLE_MAX_AREA)\n",
    "union_noholes = remove_small_holes(union_geom, HOLE_MAX_AREA)\n",
    "gdf_union_noholes = gpd.GeoDataFrame(geometry=[union_noholes], crs=CRS_TARGET)\n",
    "save_layer(gdf_union_noholes, \"4_union_noholes\")\n",
    "\n",
    "# ------------ 5) Negative buffer\n",
    "logging.info(\"Applying negative buffer %.2fm…\", BUF_NEG)\n",
    "contracted = union_noholes.buffer(BUF_NEG)\n",
    "gdf_contracted = gpd.GeoDataFrame(geometry=[contracted], crs=CRS_TARGET)\n",
    "save_layer(gdf_contracted, \"5_contracted\")\n",
    "\n",
    "# ------------ 6) Multipart → singlepart\n",
    "logging.info(\"Exploding multipart geometries…\")\n",
    "gs_single = gpd.GeoSeries([contracted], crs=f\"EPSG:{CRS_TARGET}\").explode(index_parts=False).reset_index(drop=True)\n",
    "gdf_single = gpd.GeoDataFrame(geometry=gs_single, crs=f\"EPSG:{CRS_TARGET}\")\n",
    "save_layer(gdf_single, \"6_exploded\")\n",
    "\n",
    "# ------------ 7) Keep largest polygon & simplify\n",
    "gdf_single[\"area_m2\"] = gdf_single.geometry.area\n",
    "gdf_top1 = gdf_single.sort_values(\"area_m2\", ascending=False).head(1).reset_index(drop=True)\n",
    "save_layer(gdf_top1, \"7_top1\")\n",
    "\n",
    "logging.info(\"Simplifying geometry (tolerance=0.5)…\")\n",
    "gdf_top1[\"geometry\"] = gdf_top1.geometry.simplify(tolerance=0.5, preserve_topology=True)\n",
    "save_layer(gdf_top1, \"7_simplified\")\n",
    "\n",
    "# ------------ 8) Write final output (always)\n",
    "logging.info(\"Writing final result: %s :: %s\", OUT_GPKG, OUT_LAYER)\n",
    "gdf_top1.to_file(OUT_GPKG, layer=OUT_LAYER, driver=\"GPKG\")\n",
    "logging.info(\"Done. %d polygon(s) written.\", len(gdf_top1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a07633c",
   "metadata": {},
   "source": [
    "No. 9 I will do by a QGIS Plugin calles BecaGIS. (via qgis_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b494dedf",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------\n",
      "Inputs\n",
      "----------------\n",
      "\n",
      "INPUT:\t.\\data\\working_data\\zurich_walk_network_union.gpkg\n",
      "OUTPUT:\t.\\data\\working_data\\zurich_walking_network_final.shp\n",
      "POSTPROCESSING:\t0\n",
      "\n",
      "\n",
      "\n",
      "----------------\n",
      "Results\n",
      "----------------\n",
      "\n",
      "OUTPUT:\t.\\data\\working_data\\zurich_walking_network_final.shp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\claud\\anaconda3\\envs\\conda-qgis\\Lib\\site-packages\\PyPDF2\\__init__.py:21: DeprecationWarning: PyPDF2 is deprecated. Please move to the pypdf library instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# ------------ 9) Skeletonize the network\n",
    "!qgis_process run becagis:Skeleton -- INPUT=.\\data\\working_data\\zurich_walk_network_union.gpkg POSTPROCESSING=0 OUTPUT=.\\data\\working_data\\zurich_walking_network_final.shp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad518e4",
   "metadata": {},
   "source": [
    "## No. 10: Spatial Join with OSM Attributes\n",
    "(Outdated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca73f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 20:51:27,863 - INFO - Created 1,183,576 records\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: ./data/zurich_walking_network_final.gpkg (layer='network_segments')\n",
      "Test area: 3.14 km² | Duration: 29349.99 seconds | Duration for 1 km²: 9342.40 seconds\n"
     ]
    }
   ],
   "source": [
    "# ------------ 10) Join layers according to midpoints\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import LineString, Point, MultiPoint\n",
    "from shapely.ops import snap, unary_union\n",
    "\n",
    "# --- Pfade & Parameter\n",
    "LAYER1_PATH = r\"./data/working_data/zurich_walking_network_final.shp\"\n",
    "LAYER2_GPKG = TEST_GPKG\n",
    "LAYER2_NAME = \"1_osm_edges\"\n",
    "OUT_GPKG = r\"./data/zurich_walking_network_final.gpkg\"\n",
    "OUT_LAYER = \"network_segments\"\n",
    "TARGET_CRS = \"EPSG:2056\"\n",
    "MAX_DISTANCE_METERS = 10\n",
    "DROP_COLS = [\"fid\", \"area_m2\"]\n",
    "\n",
    "# --- Parameter fuer Netzreparatur\n",
    "USE_PREP = True\n",
    "SNAP_TOL_M = 2\n",
    "MIN_SEGMENT_LEN = 0\n",
    "\n",
    "def endpoints_multipoint(gdf):\n",
    "    pts = []\n",
    "    for geom in gdf.geometry:\n",
    "        if geom and not geom.is_empty:\n",
    "            if geom.geom_type == \"LineString\":\n",
    "                cs = list(geom.coords)\n",
    "                if len(cs) >= 2:\n",
    "                    pts.extend([Point(cs[0]), Point(cs[-1])])\n",
    "            elif geom.geom_type == \"MultiLineString\":\n",
    "                for ls in geom.geoms:\n",
    "                    cs = list(ls.coords)\n",
    "                    if len(cs) >= 2:\n",
    "                        pts.extend([Point(cs[0]), Point(cs[-1])])\n",
    "    return MultiPoint(pts) if pts else MultiPoint()\n",
    "\n",
    "def prepare_network_shapely(gdf: gpd.GeoDataFrame, snap_tol=1.0) -> gpd.GeoDataFrame:\n",
    "    g = gdf.copy()\n",
    "    g = g[g.geometry.notna()].copy()\n",
    "    protected = endpoints_multipoint(g)\n",
    "\n",
    "    # Linien leicht glätten (kann auch weggelassen werden)\n",
    "    g[\"geometry\"] = g.geometry.simplify(snap_tol)\n",
    "\n",
    "    # Zuruecksnappen auf geschuetzte Endpunkte\n",
    "    g[\"geometry\"] = g.geometry.apply(lambda geom: snap(geom, protected, snap_tol))\n",
    "\n",
    "    # Re-noden (alle Kreuzungen in echte Schnittkanten verwandeln)\n",
    "    unioned = unary_union(g.geometry.values)\n",
    "    if unioned.geom_type == \"MultiLineString\":\n",
    "        lines = list(unioned.geoms)\n",
    "    elif unioned.geom_type == \"LineString\":\n",
    "        lines = [unioned]\n",
    "    else:\n",
    "        lines = []\n",
    "\n",
    "    out = gpd.GeoDataFrame(geometry=lines, crs=g.crs)\n",
    "    # Mini-Segmente raus\n",
    "    out[\"length\"] = out.length\n",
    "    out = out[out[\"length\"] > MIN_SEGMENT_LEN].drop(columns=\"length\")\n",
    "    return out\n",
    "\n",
    "def explode_to_lines(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    g = gdf[gdf.geometry.notna()].copy().explode(index_parts=False)\n",
    "    return g.loc[g.geometry.type == \"LineString\"].copy()\n",
    "\n",
    "def lines_to_segments(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    rows = []\n",
    "    for _, row in gdf.iterrows():\n",
    "        coords = list(row.geometry.coords)\n",
    "        for i in range(len(coords) - 1):\n",
    "            rows.append({**row.drop(labels=[\"geometry\"]).to_dict(),\n",
    "                         \"geometry\": LineString([coords[i], coords[i+1]])})\n",
    "    segs = gpd.GeoDataFrame(rows, geometry=\"geometry\", crs=gdf.crs)\n",
    "    segs.reset_index(drop=True, inplace=True)\n",
    "    return segs\n",
    "\n",
    "def midpoint(ls: LineString) -> Point:\n",
    "    return ls.interpolate(0.5, normalized=True)\n",
    "\n",
    "def drop_if_present(df, cols):\n",
    "    return df[[c for c in df.columns if c not in cols]]\n",
    "\n",
    "# --- Load + CRS\n",
    "layer1 = gpd.read_file(LAYER1_PATH)\n",
    "layer2 = gpd.read_file(LAYER2_GPKG, layer=LAYER2_NAME)\n",
    "if layer1.crs != TARGET_CRS: layer1 = layer1.to_crs(TARGET_CRS)\n",
    "if layer2.crs != TARGET_CRS: layer2 = layer2.to_crs(TARGET_CRS)\n",
    "\n",
    "# --- Preprocessing fuer Layer 1\n",
    "if USE_PREP:\n",
    "    layer1 = prepare_network_shapely(layer1, snap_tol=SNAP_TOL_M)\n",
    "\n",
    "# 1) Segmente aus Layer 1\n",
    "layer1_segments = lines_to_segments(explode_to_lines(layer1))\n",
    "layer1_segments[\"seg_id\"] = layer1_segments.index\n",
    "\n",
    "# 2–3) Layer 2 vorbereiten + Midpoints\n",
    "layer2_lines = explode_to_lines(layer2).copy()\n",
    "layer2_lines[\"edge_id\"] = layer2_lines.index\n",
    "seg_mid = gpd.GeoDataFrame(layer1_segments[[\"seg_id\"]].copy(),\n",
    "                           geometry=layer1_segments.geometry.apply(midpoint),\n",
    "                           crs=layer1_segments.crs)\n",
    "edge_mid = gpd.GeoDataFrame(layer2_lines[[\"edge_id\"]].copy(),\n",
    "                            geometry=layer2_lines.geometry.apply(midpoint),\n",
    "                            crs=layer2_lines.crs)\n",
    "\n",
    "# 4) Nearest match (Midpoint → Midpoint)\n",
    "joined_pts = gpd.sjoin_nearest(\n",
    "    seg_mid, edge_mid, how=\"left\",\n",
    "    max_distance=MAX_DISTANCE_METERS,\n",
    "    distance_col=\"dist_mid2mid\",\n",
    "    lsuffix=\"_seg\", rsuffix=\"_edge\"\n",
    ")\n",
    "\n",
    "# 5) Mapping + Edge-Attribute mergen\n",
    "mapping = joined_pts[[\"seg_id\", \"edge_id\", \"dist_mid2mid\"]]\n",
    "out = layer1_segments.merge(mapping, on=\"seg_id\", how=\"left\")\n",
    "edge_attrs = layer2_lines.drop(columns=[\"geometry\"])\n",
    "out = out.merge(edge_attrs, on=\"edge_id\", how=\"left\", suffixes=(\"\", \"_edge\"))\n",
    "\n",
    "# 6) Clean + Save\n",
    "out = drop_if_present(out, DROP_COLS)\n",
    "out.to_file(OUT_GPKG, driver=\"GPKG\", layer=OUT_LAYER, index=False)\n",
    "print(f\"Wrote: {OUT_GPKG} (layer='{OUT_LAYER}')\")\n",
    "import winsound\n",
    "winsound.Beep(800, 200)\n",
    "winsound.Beep(1200, 400)\n",
    "winsound.Beep(1600, 400)\n",
    "\n",
    "END_TIME = time.time()\n",
    "DURATION = END_TIME - START\n",
    "AREA_KM2 = (3.14159 * TEST_AREA * TEST_AREA) / 1_000_000\n",
    "print(f\"Test area: {AREA_KM2:.2f} km² | Duration: {DURATION:.2f} seconds | Duration for 1 km²: {DURATION / AREA_KM2:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a13b49",
   "metadata": {},
   "source": [
    "## No. 11: Remove dead-end segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf47270",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\claud\\AppData\\Local\\Temp\\ipykernel_8064\\1883461389.py:50: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mask_stubs = (gdf[\"is_leaf\"].fillna(False)) & (gdf[\"length_m\"] < min_len)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: removed 1078427 segments, remaining 105147\n",
      "Iteration 2: removed 2558 segments, remaining 102589\n",
      "Total removed after 2 iterations: 1080985\n",
      "Saved cleaned edges to layer 'network_segments_cleaned' in ./data/zurich_walking_network_final.gpkg\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------\n",
    "# Iteratively remove short stubs (leaf segments) OR null-degree segments\n",
    "# (N iterations, robust to column overlaps)\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "OUT_GPKG = r\"./data/zurich_walking_network_final.gpkg\"\n",
    "OUT_LAYER = \"network_segments\"\n",
    "EDGES_CLEAN_LAYER = \"network_segments_cleaned\"\n",
    "\n",
    "MIN_LEN = 50  # meters (Stub-Threshold)\n",
    "N_ITER = 2     # number of iterations\n",
    "\n",
    "def clean_once(gdf, min_len):\n",
    "    \"\"\"Eine Iteration: degrees berechnen, Stummel < min_len oder Null-Degrees entfernen.\"\"\"\n",
    "    # Index resetten -> 'idx' passt sicher zum DataFrame\n",
    "    gdf = gdf.reset_index(drop=True).copy()\n",
    "\n",
    "    # alte Spalten weg, falls vorhanden (verhindert Overlap)\n",
    "    for col in [\"deg_u\", \"deg_v\", \"is_leaf\", \"length_m\"]:\n",
    "        if col in gdf.columns:\n",
    "            gdf.drop(columns=col, inplace=True)\n",
    "\n",
    "    # Graph aus Start-/Endpunkten bauen\n",
    "    G = nx.Graph()\n",
    "    for idx, geom in enumerate(gdf.geometry):\n",
    "        coords = list(geom.coords)\n",
    "        if len(coords) >= 2:\n",
    "            start, end = coords[0], coords[-1]\n",
    "            G.add_edge(start, end, idx=idx)\n",
    "\n",
    "    # Degrees\n",
    "    deg = dict(G.degree())\n",
    "    edges_df = nx.to_pandas_edgelist(G)\n",
    "    edges_df[\"deg_u\"] = edges_df[\"source\"].map(deg)\n",
    "    edges_df[\"deg_v\"] = edges_df[\"target\"].map(deg)\n",
    "    edges_df[\"is_leaf\"] = (edges_df[\"deg_u\"] == 1) | (edges_df[\"deg_v\"] == 1)\n",
    "\n",
    "    # Join zurück (per idx)\n",
    "    deg_map = edges_df.set_index(\"idx\")[[\"deg_u\", \"deg_v\", \"is_leaf\"]]\n",
    "    gdf = gdf.join(deg_map, how=\"left\")\n",
    "\n",
    "    # Länge (achte auf metrisches CRS – falls nicht, vorher .to_crs(2056))\n",
    "    gdf[\"length_m\"] = gdf.geometry.length\n",
    "\n",
    "    # Löschmasken\n",
    "    mask_stubs = (gdf[\"is_leaf\"].fillna(False)) & (gdf[\"length_m\"] < min_len)\n",
    "    mask_nulls = (gdf[\"deg_u\"].isna()) | (gdf[\"deg_v\"].isna())\n",
    "\n",
    "    to_remove_idx = gdf.index[mask_stubs | mask_nulls]\n",
    "    gdf_clean = gdf.drop(index=to_remove_idx)\n",
    "\n",
    "    return gdf_clean, int(len(to_remove_idx))\n",
    "\n",
    "# ------------------- MAIN -------------------\n",
    "gdf = gpd.read_file(OUT_GPKG, layer=OUT_LAYER)\n",
    "gdf = gdf[gdf.geometry.type == \"LineString\"].copy()\n",
    "\n",
    "# Falls nötig, in metrisches CRS (für korrekte Meter-Längen)\n",
    "if (gdf.crs is None) or gdf.crs.is_geographic:\n",
    "    gdf = gdf.to_crs(2056)  # LV95\n",
    "\n",
    "total_removed = 0\n",
    "for i in range(N_ITER):\n",
    "    gdf, removed = clean_once(gdf, MIN_LEN)\n",
    "    total_removed += removed\n",
    "    print(f\"Iteration {i+1}: removed {removed} segments, remaining {len(gdf)}\")\n",
    "    if removed == 0:\n",
    "        break\n",
    "\n",
    "print(f\"Total removed after {i+1} iterations: {total_removed}\")\n",
    "\n",
    "# Export\n",
    "gdf.to_file(OUT_GPKG, driver=\"GPKG\", layer=EDGES_CLEAN_LAYER, index=False)\n",
    "print(f\"Saved cleaned edges to layer '{EDGES_CLEAN_LAYER}' in {OUT_GPKG}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6747c",
   "metadata": {},
   "source": [
    "## Compute Network Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40e1d76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 11:46:18,861 - INFO - Processing OSM Filtered ...\n",
      "2025-10-13 11:46:23,312 - INFO - Processing Stadt Zurich ...\n",
      "2025-10-13 11:46:24,782 - INFO - Processing Final Cleaned ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Network Graph Statistics ===\n",
      "                n_edges  n_nodes  total_length_km  n_components  \\\n",
      "OSM Filtered    41409.0  57837.0           1939.7       16985.0   \n",
      "Stadt Zurich    30763.0  21046.0           1864.4          47.0   \n",
      "Final Cleaned  102590.0  83712.0           2353.4           2.0   \n",
      "\n",
      "               largest_component_nodes  mean_degree  \n",
      "OSM Filtered                    4238.0         1.43  \n",
      "Stadt Zurich                   20942.0         2.92  \n",
      "Final Cleaned                  83708.0         2.45  \n",
      "\n",
      "Saved to ./data/working_data/network_graph_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Compute network graph statistics for 3 GeoPackages\n",
    "# ============================================================\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from shapely.geometry import LineString, MultiLineString\n",
    "import logging\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "CRS_TARGET = 2056  # LV95 coordinate system\n",
    "\n",
    "# Replace these with your actual file paths:\n",
    "FILES = {\n",
    "    \"OSM Filtered\": \"./data/working_data/is_test.gpkg\",\n",
    "    \"Stadt Zurich\": \"./data/working_data/is_test.gpkg\",\n",
    "    \"Final Cleaned\": r\"D:\\Masterarbeit\\03_Model\\Scripts\\3_Classificatiaon\\3_Classification\\data\\network_segments_with_predictions_full.parquet\",\n",
    "}\n",
    "\n",
    "# Replace these with the correct layer names in each GPKG:\n",
    "LAYERS = {\n",
    "    \"OSM Filtered\": \"1_osm_edges\",\n",
    "    \"Stadt Zurich\": \"1_zrh_edges\",\n",
    "    \"Final Cleaned\": \"network_segments_with_predictions_full\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ------------------------------------------------------------\n",
    "def get_endpoints(geom):\n",
    "    \"\"\"Return start and end coordinates of LineString or MultiLineString.\"\"\"\n",
    "    if isinstance(geom, LineString):\n",
    "        coords = list(geom.coords)\n",
    "        return coords[0], coords[-1]\n",
    "    elif isinstance(geom, MultiLineString):\n",
    "        all_coords = [list(line.coords) for line in geom.geoms if len(line.coords) > 1]\n",
    "        if not all_coords:\n",
    "            return None, None\n",
    "        flat = [pt for line in all_coords for pt in line]\n",
    "        return flat[0], flat[-1]\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def compute_network_stats(gdf):\n",
    "    \"\"\"Compute simple network statistics from a GeoDataFrame.\"\"\"\n",
    "    # Keep only line geometries\n",
    "    gdf = gdf[gdf.geometry.type.isin([\"LineString\", \"MultiLineString\"])].copy()\n",
    "    if len(gdf) == 0:\n",
    "        return None\n",
    "\n",
    "    gdf = gdf.to_crs(CRS_TARGET)\n",
    "    gdf[\"length_m\"] = gdf.geometry.length\n",
    "    gdf[\"endpoints\"] = gdf.geometry.apply(get_endpoints)\n",
    "    gdf = gdf.dropna(subset=[\"endpoints\"])\n",
    "\n",
    "    # Build undirected graph\n",
    "    G = nx.Graph()\n",
    "    for idx, (s, e) in enumerate(gdf[\"endpoints\"]):\n",
    "        if (s is None) or (e is None):\n",
    "            continue\n",
    "        G.add_edge(s, e, fid=idx, length=gdf.iloc[idx][\"length_m\"])\n",
    "\n",
    "    # Compute metrics\n",
    "    n_edges = G.number_of_edges()\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_components = nx.number_connected_components(G)\n",
    "    largest_cc = max(nx.connected_components(G), key=len) if n_components > 0 else []\n",
    "    mean_degree = sum(dict(G.degree()).values()) / n_nodes if n_nodes > 0 else 0\n",
    "    total_length_km = gdf[\"length_m\"].sum() / 1000\n",
    "\n",
    "    return {\n",
    "        \"n_edges\": n_edges,\n",
    "        \"n_nodes\": n_nodes,\n",
    "        \"total_length_km\": round(total_length_km, 1),\n",
    "        \"n_components\": n_components,\n",
    "        \"largest_component_nodes\": len(largest_cc),\n",
    "        \"mean_degree\": round(mean_degree, 2),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# MAIN EXECUTION\n",
    "# ------------------------------------------------------------\n",
    "results = {}\n",
    "for name in FILES.keys():\n",
    "    try:\n",
    "        logging.info(f\"Processing {name} ...\")\n",
    "        fp = FILES[name]\n",
    "\n",
    "        # special case: parquet\n",
    "        if fp.endswith(\".parquet\"):\n",
    "            gdf = gpd.read_parquet(fp)\n",
    "        else:\n",
    "            gdf = gpd.read_file(fp, layer=LAYERS[name])\n",
    "\n",
    "        stats = compute_network_stats(gdf)\n",
    "        if stats:\n",
    "            results[name] = stats\n",
    "        else:\n",
    "            logging.warning(f\"No valid line geometries in {name}.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {name}: {e}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# EXPORT RESULTS\n",
    "# ------------------------------------------------------------\n",
    "if results:\n",
    "    df = pd.DataFrame(results).T\n",
    "    df.to_csv(\"./data/working_data/network_graph_stats.csv\")\n",
    "    print(\"\\n=== Network Graph Statistics ===\")\n",
    "    print(df)\n",
    "    print(\"\\nSaved to ./data/working_data/network_graph_stats.csv\")\n",
    "else:\n",
    "    print(\"⚠️ No valid results computed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics-master-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
